{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run once to install all dependencies.. then comment out..\n",
    "# uncomment the following line to install/upgrade the PixieDust library\n",
    "# ! pip install pixiedust\n",
    "# ! pip install watson_developer_cloud --user\n",
    "# ! pip install pandas\n",
    "# ! pip install matplotlib\n",
    "# ! pip install cloudant\n",
    "# ! pip install bokeh==0.12.6\n",
    "# ! pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports.... Run this each time after restarting the Kernel\n",
    "import pixiedust\n",
    "import json\n",
    "import datetime\n",
    "from watson_developer_cloud import ConversationV1\n",
    "import json\n",
    "import time\n",
    "import pandas\n",
    "credentials = json.loads(open('../../credentials.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run this cell each time after restarting the Kernel\n",
    "from cloudant.client import Cloudant\n",
    "\n",
    "class cloudant_database(object):\n",
    "    def __init__(self, db_name):\n",
    "        self.db_name = db_name\n",
    "        self.db = None\n",
    "        self.client = None\n",
    "\n",
    "    def login(self, username, password, url):\n",
    "        self.client = Cloudant(username, password, url=url, connect=True)\n",
    "\n",
    "    def create_database(self):\n",
    "        self.db = self.client.create_database(self.db_name)\n",
    "\n",
    "    def list_documents(self, prefix):\n",
    "        docs = []\n",
    "        for document in self.db:\n",
    "            name = document['_id']\n",
    "            #print(name)\n",
    "            if name and name.startswith(prefix):\n",
    "                docs.append(document)\n",
    "        return docs\n",
    "\n",
    "    def store_json_log(self, data, doc_id):\n",
    "        try:\n",
    "            self.db[doc_id]\n",
    "        except:\n",
    "            self.db.create_document({\"_id\":doc_id, \"data\": data})\n",
    "            print('document created')\n",
    "        else:\n",
    "            print('document exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdb = cloudant_database('conversation-analysis')\n",
    "cdb.login(credentials['cloudant']['username'], credentials['cloudant']['password'], credentials['cloudant']['url'])\n",
    "cdb.create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_baseline(prefix):\n",
    "    utterances = cdb.list_documents(prefix)\n",
    "    baseline = utterances[len(utterances) - 1]\n",
    "    baseline = baseline['data']\n",
    "    return baseline\n",
    "\n",
    "def create_experiment(prefix):\n",
    "    ex = cdb.list_documents(prefix)\n",
    "    experiment_results = ex[len(ex) - 1]\n",
    "    experiment_results = experiment_results['data']\n",
    "    return experiment_results\n",
    "\n",
    "def mark_irrelevant(baseline):\n",
    "    for row in baseline:\n",
    "        if len(row['intents']) == 0:\n",
    "            row['intents'].append({'intent' : 'irrelevant', 'confidence' : 100})\n",
    "    return baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = create_baseline(\"utterances_\")\n",
    "baseline = mark_irrelevant(baseline_1)\n",
    "# print(baseline)\n",
    "experiment_results = create_experiment(\"experiment_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline = create_baseline(\"utterances_\")\n",
    "# baseline = mark_irrelevant(baseline_1)\n",
    "# # print(baseline)\n",
    "# experiment_results = create_experiment(\"experiment_\")\n",
    "conversation_local = ConversationV1(\n",
    "    username=credentials['conversation']['username'],\n",
    "    password=credentials['conversation']['password'],\n",
    "    version='2017-05-26')\n",
    "\n",
    "conversation_prod_ws = credentials['conversation']['workspace_id_prod']\n",
    "conversation_test_ws = credentials['conversation']['workspace_id_test']\n",
    "\n",
    "prod_ws = conversation_local.get_workspace(workspace_id=conversation_prod_ws, export=True)\n",
    "test_ws = conversation_local.get_workspace(workspace_id=conversation_test_ws, export=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count how many times each intent was used per experiment run\n",
    "def count_intents(list_name, count, prefix):\n",
    "    for row in list_name:\n",
    "        name = row['intents'][0]['intent']\n",
    "        if name not in count:\n",
    "            count[name] = {'baseline' : 0, 'experiment' : 0}\n",
    "        keys = count[name].keys()\n",
    "        count[name][prefix] = count[name][prefix] + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_1={}\n",
    "count_1 = count_intents(baseline, count_1,'baseline')\n",
    "count_1= count_intents(experiment_results, count_1,'experiment')\n",
    "# print(count_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a data frame for count of intents in baseline and experiments\n",
    "def create_intent_dataframe(count):\n",
    "    arr = []    \n",
    "    for key in count:\n",
    "        arr.append({'name' : key, 'baseline' : count[key]['baseline'], 'experiment' : count[key]['experiment']})\n",
    "    intent_distribution_df = pandas.DataFrame(arr)\n",
    "    return intent_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intent_distribution_df = get_intent_distribution_dataframe(count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_intent_analysis_dataframe():\n",
    "    intent_analysis = []\n",
    "    prod_examples = 0\n",
    "    test_examples = 0\n",
    "    intent_analysis.append({'intent' : 'irrelevant', 'baseline': len(prod_ws['counterexamples']), 'experiment' : len(test_ws['counterexamples'])})\n",
    "    for intent in prod_ws['intents']:\n",
    "        prod_examples = prod_examples + len(intent['examples'])\n",
    "        intent_analysis.append({'intent' : intent['intent'], 'baseline' : len(intent['examples']), 'experiment' : 0})\n",
    "    \n",
    "    for intent in test_ws['intents']:\n",
    "        test_examples = test_examples + len(intent['examples'])\n",
    "        found = False\n",
    "        for intnt in intent_analysis:\n",
    "            if intnt['intent'] == intent['intent']:\n",
    "                found = True\n",
    "                intnt['experiment'] = len(intent['examples'])\n",
    "                break\n",
    "\n",
    "        if not found: \n",
    "            intent_analysis.append({'intent' : intent['intent'], 'baseline' : 0, 'experiment' : len(intent['examples'])})\n",
    "    intent_analysis_df = pandas.DataFrame(intent_analysis)\n",
    "#     print(test_examples)\n",
    "#     print(prod_examples)\n",
    "#     print(intent_analysis)\n",
    "    return intent_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_examples(workspace):\n",
    "    examples = 0\n",
    "    for intent in workspace['intents']:\n",
    "        examples = examples + len(intent['examples'])\n",
    "    return examples\n",
    "\n",
    "def count_val_syn(workspace):\n",
    "    values = 0\n",
    "    synonyms = 0\n",
    "    for entity in workspace['entities']:\n",
    "        values = values + len(entity['values'])\n",
    "        for value in entity['values']:\n",
    "            synonyms = synonyms + len(value['synonyms'])\n",
    "    return (values, synonyms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count_examples(test_ws))\n",
    "# print(count_examples(prod_ws))\n",
    "# print(count_val_syn(test_ws)[0])\n",
    "# print(count_val_syn(test_ws)[1])\n",
    "# print(count_val_syn(prod_ws)[0])\n",
    "# print(count_val_syn(prod_ws)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ws_analysis_dataframe():\n",
    "    ws_analysis = [] \n",
    "    ws_analysis.append({'name' : 'intents', 'baseline' : len(prod_ws['intents']), 'experiment' : len(test_ws['intents'])})\n",
    "    ws_analysis.append({'name' : 'entities', 'baseline' : len(prod_ws['entities']), 'experiment' : len(test_ws['entities'])})\n",
    "    ws_analysis.append({'name' : 'dialog_nodes', 'baseline' : len(prod_ws['dialog_nodes']), 'experiment' : len(test_ws['dialog_nodes'])})\n",
    "    ws_analysis.append({'name' : 'counterexamples', 'baseline' : len(prod_ws['counterexamples']), 'experiment' : len(test_ws['counterexamples'])})\n",
    "    \n",
    "    prod_examples = count_examples(prod_ws)\n",
    "    test_examples = count_examples(test_ws)\n",
    "    ws_analysis.append({'name' : 'examples', 'baseline' : prod_examples, 'experiment' : test_examples})\n",
    "    \n",
    "    prod_values = count_val_syn(prod_ws)[0]\n",
    "    prod_synonyms = count_val_syn(prod_ws)[1]\n",
    "    test_values = count_val_syn(test_ws)[0]\n",
    "    test_synonyms = count_val_syn(test_ws)[1]\n",
    "    \n",
    "    ws_analysis.append({'name' : 'values', 'baseline' : prod_values, 'experiment' : test_values})\n",
    "    ws_analysis.append({'name' : 'synonyms', 'baseline' : prod_synonyms, 'experiment' : test_synonyms})\n",
    "    \n",
    "    \n",
    "    ws_analysis_df = pandas.DataFrame(ws_analysis)\n",
    "    return ws_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intent_analysis_df = create_intent_analysis_dataframe()\n",
    "ws_analysis_df = create_ws_analysis_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a data frame with the original utterance, the intent from each run, the confidence, whether or not the \n",
    "#intent changed, and the confidence delta\n",
    "\n",
    "def create_delta(baseline, experiment_results):\n",
    "    merged1 = [{\"text\": b[\"text\"], \"intent_1\" : b['intents'][0]['intent'], \n",
    "           \"conf_1\" : b['intents'][0]['confidence']} for b in baseline]\n",
    "    merged2 =[{\"text\": b[\"text\"], \"intent_2\" : b['intents'][0]['intent'], \n",
    "           \"conf_2\" : b['intents'][0]['confidence']} for b in experiment_results]\n",
    "    merged = list(zip(merged1, merged2))\n",
    "\n",
    "    merged = [{'intent_1': b[0]['intent_1'], 'conf_1': b[0]['conf_1'],\n",
    "          'intent_2': b[1]['intent_2'], 'conf_2': b[1]['conf_2'], \n",
    "          'intent_changed' : b[0]['intent_1'] != b[1]['intent_2'],\n",
    "          'conf_delta' : b[1]['conf_2'] - b[0]['conf_1'], 'text' : b[0]['text']} for b in merged]\n",
    "    experiment_run_dataframe = pandas.DataFrame(merged)\n",
    "    experiment_run_dataframe = experiment_run_dataframe.loc[experiment_run_dataframe['intent_changed']==True]\n",
    "    return experiment_run_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "name",
      "rowCount": "500",
      "valueFields": "baseline,experiment"
     }
    }
   },
   "outputs": [],
   "source": [
    "experiment_run_dataframe = create_delta(baseline, experiment_results)\n",
    "\n",
    "display(ws_analysis_df)\n",
    "display(intent_analysis_df)\n",
    "display(intent_distribution_df)\n",
    "display(experiment_run_dataframe)\n",
    "    \n",
    "\n",
    "#pdf = pdf.loc[pdf['conf_delta']>0.1]\n",
    "#display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
